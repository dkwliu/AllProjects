{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84f61214a6e7168e6739662f030ce5ef",
     "grade": false,
     "grade_id": "cell-f49b8ea35a2dabd9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Project-1:-Text-Processing\" data-toc-modified-id=\"Project-1:-Text-Processing-1\">Project 1: Text Processing</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#By-the-end-of-this-project,-you-should-be-able-to:\" data-toc-modified-id=\"By-the-end-of-this-project,-you-should-be-able-to:-2.0.1\">By the end of this project, you should be able to:</a></span></li></ul></li></ul></li><li><span><a href=\"#Task-1.-Load-data-\" data-toc-modified-id=\"Task-1.-Load-data--3\">Task 1. Load data </a></span></li><li><span><a href=\"#Task-2.-Data-cleaning\" data-toc-modified-id=\"Task-2.-Data-cleaning-4\">Task 2. Data cleaning</a></span></li><li><span><a href=\"#Task-3-Count-words\" data-toc-modified-id=\"Task-3-Count-words-5\">Task 3 Count words</a></span></li><li><span><a href=\"#Task-4.-Calculate-probabilities-of-individual-words\" data-toc-modified-id=\"Task-4.-Calculate-probabilities-of-individual-words-6\">Task 4. Calculate probabilities of individual words</a></span></li><li><span><a href=\"#Task-5.-Calculate-probabilities-of-a-group-of-words\" data-toc-modified-id=\"Task-5.-Calculate-probabilities-of-a-group-of-words-7\">Task 5. Calculate probabilities of a group of words</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-8\">Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "08cc5f44453dc9e2c4e910fbd944e33e",
     "grade": false,
     "grade_id": "cell-588eb434a3567ee2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Project 1: Text Processing\n",
    "-----\n",
    "\n",
    "Text is a very valuable source of data. Think of the information in books, reports, emails, and on websites. However, text data can be difficult to extract that value. There are many reasons. Text is less-structured than tabular data. Also, data in string format is harder to process for a computer than data in numerical format. \n",
    "\n",
    "The first step in most Natural Language Processing projects is text processing. This project guide you through implementing fundamental text processing algorithms from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "27435cf203c1eb18972ede8239330525",
     "grade": false,
     "grade_id": "cell-5cc85b93d38670f4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Learning Outcomes\n",
    "----\n",
    "\n",
    "#### By the end of this project, you should be able to:\n",
    "\n",
    "- Work with text data in Python.\n",
    "- Efficiently process large amounts of text data.\n",
    "- Apply your knowledge of Python: assignments, expressions, if and loop statements, functions, lists, and libraries. \n",
    "- Write the following text processing functions from scratch:\n",
    "    1. Load automatically load data from subfolders.\n",
    "    2. Split text into separate words.\n",
    "    3. Count the occurrence of words.\n",
    "    4. Calculate probabilities of individual words\n",
    "    5. Calculate probabilities of a group of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89568de670d8936b162de23fc95062ad",
     "grade": false,
     "grade_id": "cell-b3989d3eb2c9086b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1833cb7334245c8ec5c779255eabebf2",
     "grade": false,
     "grade_id": "cell-a2df962d5b2db997",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Task 1. Load data \n",
    "-----\n",
    "\n",
    "You'll need a corpus for NLP, a corpus is a collection of related documents. \n",
    "\n",
    "For this project, we are going to use a collection of [slate.com](https://slate.com) articles. The data has already be downloaded for you.\n",
    "\n",
    "For example, in folder `./text/1` there is `Article247_4.txt`. The web version is [here](https://slate.com/culture/1999/03/harmonic-convergences.html).\n",
    "\n",
    "The first function is the `load_data` function.\n",
    "\n",
    "Write code that implements the following steps:\n",
    "\n",
    "1. We need to efficiently and effectively walk all of the subfolders and load all of the text data. The best solution is from [pathlib](https://docs.python.org/3/library/pathlib.html).\n",
    "\n",
    "1. As we do that, create an index that keeps tracks of document name. Tracking meta-data is very important in a data science project.\n",
    "\n",
    "1. For each document, remove all linebreaks - `\"\\n\"` and split the continuous string into separate words (called tokenization) using only `str` methods. A mature NLP system would use an established text processing package or regular expression. For now, `str` methods will work.\n",
    "\n",
    "As always, do not import anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e76071f59e6b1695146d317811187be1",
     "grade": false,
     "grade_id": "cell-31799ff4cbe87441",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path('./text/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3ccd0a622db24287001b0747a8f27d3e",
     "grade": false,
     "grade_id": "cell-e20867747654e8d9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from os import PathLike\n",
    "from typing import List\n",
    "\n",
    "def load_data(path: PathLike) -> (List[PathLike], List[List[str]]):\n",
    "\n",
    "    title_index = []\n",
    "    docs = []\n",
    "    \n",
    "    directory = path\n",
    "    \n",
    "    for each in directory.glob('**/*.txt'):\n",
    "        with open(each) as title:\n",
    "            line = each\n",
    "            title_index.append(line)\n",
    "            \n",
    "            content = title.read().split()\n",
    "            docs.append(content)\n",
    "    \n",
    "    return title_index, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "20c1c29c949358eb19b7eba1ae86cb4d",
     "grade": true,
     "grade_id": "cell-8c5f18b22e696a31",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test code for load_data function.\n",
    "5 points\n",
    "This cell should NOT give any errors when it is run.\n",
    "\"\"\"\n",
    "\n",
    "# Run function to load documents into memory\n",
    "title_index, docs = load_data(path)\n",
    "\n",
    "# Sort both lists by path, aka title_index\n",
    "title_index, docs = [list(x) for x in zip(*sorted(zip(title_index, docs), key=lambda pair: pair[0]))]\n",
    "\n",
    "n_docs = 4_530 \n",
    "assert len(docs)   == n_docs\n",
    "assert len(title_index) == n_docs\n",
    "\n",
    "i = 0 \n",
    "assert str(title_index[i]) == 'text/1/Article247_4.txt'\n",
    "assert len(docs[i]) == 586\n",
    "assert docs[i][:10] == ['Harmonic',\n",
    "                         'Convergences',\n",
    "                         \"You're\",\n",
    "                         'right,',\n",
    "                         \"Maxim's\",\n",
    "                         'strong',\n",
    "                         'point',\n",
    "                         'is',\n",
    "                         'that',\n",
    "                         \"it's\"]\n",
    "i = -1 \n",
    "assert str(title_index[i]) == 'text/9/Article247_3299.txt'\n",
    "assert len(docs[i]) == 756\n",
    "assert docs[i][-10:] == ['of',\n",
    "                         'some',\n",
    "                         'future',\n",
    "                         '14-volume',\n",
    "                         'biography',\n",
    "                         'of',\n",
    "                         'Rosalynn',\n",
    "                         'Carter.',\n",
    "                         'Yours,',\n",
    "                         'Chris']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e1e3523167ef796c923a72be35685376",
     "grade": false,
     "grade_id": "cell-b15cfd9047daee70",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Task 2. Data cleaning\n",
    "-----\n",
    "\n",
    "Now that we have the data loaded of disk into memory, we can process it more.\n",
    "\n",
    "Write code that implements the following:\n",
    "\n",
    "1. Lowercase all words\n",
    "1. Remove any remaining line breaks\n",
    "1. Remove punctuation\n",
    "1. Remove any empty strings (e.g., strings that were just punctuation and are now empty)\n",
    "\n",
    "Ignore apostrophes.\n",
    "\n",
    "Use only `str` processing methods.\n",
    "\n",
    "Try to write efficient code. The two best ways to write efficient code is:\n",
    "\n",
    "1. Use the best data structure for the problem. Review Python's built-in types [here](https://docs.python.org/3/library/stdtypes.html).\n",
    "1. Minimize the number of complete passes through the data. \n",
    "\n",
    "Your code should run in less than 10 seconds (my code takes less than 5 seconds on my machine). If your code times out during autograding, you'll lose a lot of points.\n",
    "\n",
    "Hints: \n",
    "\n",
    "1. Write helper functions. \n",
    "1. Then process the documents with list comprehensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5728fa1ad5137a9e29109ed29da07d9f",
     "grade": false,
     "grade_id": "cell-3094a1329ca58dad",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "newList = []\n",
    "\n",
    "def cutWord(aString):\n",
    "\n",
    "    newstring = \"\"\n",
    "\n",
    "    for each in aString:\n",
    "        if each not in punctuation:\n",
    "            newstring = newstring + each.lower()\n",
    "                 \n",
    "    return newstring\n",
    "\n",
    "for x in range(len(docs)):\n",
    "    newList = [cutWord(aString = docs[x][y]) for y in range(len(docs[x])) if docs[x][y] != None]\n",
    "    docs[x] = newList\n",
    "\n",
    "    while \"\" in docs[x]:\n",
    "        docs[x].remove(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8948d2b4189fa43cc3cefb745d715a8d",
     "grade": true,
     "grade_id": "cell-9a458bb68ab01592",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test code for data cleaning.\n",
    "5 points\n",
    "This cell should NOT give any errors when it is run.\n",
    "\"\"\"\n",
    "\n",
    "i = 0 \n",
    "assert len(docs[i]) == 586\n",
    "assert docs[i][:10] == ['harmonic',\n",
    "                         'convergences',\n",
    "                         'youre',\n",
    "                         'right',\n",
    "                         'maxims',\n",
    "                         'strong',\n",
    "                         'point',\n",
    "                         'is',\n",
    "                         'that',\n",
    "                         'its']\n",
    "i = -1 \n",
    "assert len(docs[i]) == 753\n",
    "assert docs[i][-10:] == ['of',\n",
    "                         'some',\n",
    "                         'future',\n",
    "                         '14volume',\n",
    "                         'biography',\n",
    "                         'of',\n",
    "                         'rosalynn',\n",
    "                         'carter',\n",
    "                         'yours',\n",
    "                         'chris']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "226596905c662038be8d655ce1293e8c",
     "grade": false,
     "grade_id": "cell-a45ac0851e3be3ca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Task 3 Count words\n",
    "-----\n",
    "\n",
    "Data science is mostly counting stuff (e.g., clicks, users, …). In NLP, we count words.\n",
    "\n",
    "Write a function that counts the occurrence of each word across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fcae77c26abe1603c75977a73760bde5",
     "grade": false,
     "grade_id": "cell-03e64b5d4c20bc40",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Counter\n",
    "\n",
    "def count(docs: List[List[str]]) -> Counter:\n",
    "    \n",
    "    word_counts = None\n",
    "    compWordList = []\n",
    "    \n",
    "    for each in docs:\n",
    "        while None in each:\n",
    "            each.remove(None)\n",
    "        compWordList.extend(each)\n",
    "        \n",
    "    word_counts = Counter(compWordList)\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6d18c0f21b042ab2b321a53a0cf69cfb",
     "grade": true,
     "grade_id": "cell-d50b926ec145bb85",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test code for count function\n",
    "5 points\n",
    "This cell should NOT give any errors when it is run.\n",
    "\"\"\"\n",
    "\n",
    "word_counts = count(docs)\n",
    "\n",
    "assert word_counts.most_common(3)  == [('the', 263_910), \n",
    "                                       ('of',  115_388), \n",
    "                                       ('to',  107_005)]\n",
    "assert word_counts['brian']  == 110 \n",
    "assert word_counts['lambda'] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "04f1b0ea7cbb2060bbb2aa1ff32d852a",
     "grade": false,
     "grade_id": "cell-1131700946d9c347",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Task 4. Calculate probabilities of individual words\n",
    "-----\n",
    "\n",
    "Now that we have counts, we can normalize the counts to create probabilities. Probability is the foundation of Statistics.\n",
    "\n",
    "Write a function `word_prob` that finds the probability of a given word occurring compared to all other words.\n",
    "\n",
    "__Modeling Sidebar__\n",
    "\n",
    "We are building [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function) (PMF) for the entire corpus. \n",
    "\n",
    "We could have also built a PMF per document and compared the relative frequency of words between documents. \n",
    "\n",
    "```python\n",
    "word_count_doc_0 = Counter(docs[0])\n",
    "word_count_doc_1 = Counter(docs[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6eb2e4058e06e272a0ee3bb75b0b8ed5",
     "grade": false,
     "grade_id": "cell-ff9f9d4c05b06414",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def word_prob(counter, word):\n",
    "    \n",
    "    wrdCount = counter\n",
    "    total = 0\n",
    "    \n",
    "    for each in counter:\n",
    "        total += counter[each]\n",
    "\n",
    "    return counter[word]/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "be73e61411d26a9a9f2b0650f2be1714",
     "grade": true,
     "grade_id": "cell-56f0ce313b4d8a29",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test code for word_prob function\n",
    "3 points\n",
    "This cell should NOT give any errors when it is run.\n",
    "\"\"\"\n",
    "\n",
    "from math import isclose\n",
    "\n",
    "assert isclose(word_prob(word_counts, 'the'), 0.06324061988316144)\n",
    "assert isclose(word_prob(word_counts, 'brian'), 2.6359244390692887e-05)\n",
    "assert isclose(word_prob(word_counts, 'lambda'), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ff457b77a420fb43031417363a2bc1a9",
     "grade": false,
     "grade_id": "cell-275eeabf537b63ea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Task 5. Calculate probabilities of a group of words\n",
    "-----\n",
    "\n",
    "We can calculate the probability of a sequence of words using the joint probability formula:\n",
    "\n",
    "$$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_1 w_2) \\ldots  \\times \\ldots P(w_n \\mid w_1 \\ldots w_{n-1})$$\n",
    "\n",
    "Often we make a simplifing assumption, each word is drawn from the bag *independently* of the others $P(w_2 \\mid w_1) = P(w_2)$. This is called a [bag of words model](https://en.wikipedia.org/wiki/Bag-of-words_model).  \n",
    "\n",
    "That assumption simplifies our joint probability formula to the just a product of each word:\n",
    "    \n",
    "$$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2) \\times P(w_3) \\ldots  \\times \\ldots P(w_n) = \\prod\\limits_{i}P(w_i)$$\n",
    "\n",
    "\n",
    "First, write a `product` function that multiples each item in a sequence (just like `sum` adds each word in sequence).\n",
    "\n",
    "Then, write the `words_prob` function that finds the bag-of-words probability for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2f128dc6a815e8b83eef2a2dd98c5fc7",
     "grade": false,
     "grade_id": "cell-13fa2cf9ef95a540",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "def prod(nums: Sequence[float]) -> float:\n",
    "    \n",
    "    product = 1\n",
    "    \n",
    "    for each in nums:\n",
    "        product *= each\n",
    "    \n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "904bad4120dbc2de4345c927c5b907bd",
     "grade": true,
     "grade_id": "cell-f292cd0557275a0a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test code for prod function.\n",
    "1 points\n",
    "This cell should NOT give any errors when it is run.\n",
    "\"\"\"\n",
    "\n",
    "from numpy import product\n",
    "\n",
    "assert prod([0]) == product([0]) == 0\n",
    "assert prod([1]) == product([1]) == 1 \n",
    "assert prod([-2, -3]) == prod([-2, -3]) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5ec5a890d79a404666f4d4bb212b179f",
     "grade": false,
     "grade_id": "cell-18f806c12cb1bb5f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def words_prob(words):\n",
    "    \n",
    "    total = 0\n",
    "    product = 1\n",
    "    counter = count(docs)\n",
    "    \n",
    "    for each in counter:\n",
    "        total += counter[each]\n",
    "    \n",
    "    for word in words:\n",
    "        product *= counter[word]/total\n",
    "\n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b62505cd1754e6f2b59850729b08164e",
     "grade": true,
     "grade_id": "cell-6bdc90034ad2b330",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test code for words_prob function.\n",
    "3 points\n",
    "This cell should NOT give any errors when it is run.\n",
    "\"\"\"\n",
    "\n",
    "from math import isclose\n",
    "\n",
    "word = 'the'\n",
    "assert word_prob(word_counts, word) == words_prob([word])\n",
    "\n",
    "words = 'The quick brown fox'.lower().split()\n",
    "assert isclose(words_prob(words), 3.238175022726325e-14)\n",
    "\n",
    "words = 'The quick brown house-elf'.lower().split()\n",
    "assert words_prob(words) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "809ea61335d9cee7f76cffafd118d793",
     "grade": false,
     "grade_id": "cell-ff2d070e25115ec4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "We wrote a series of functions that takes in collection of documents, processes the documents, and calculates descriptive statistics.\n",
    "\n",
    "Give the modularity of the code, we could apply the same functions to another corpus, for New York Times articles or emails. We would have to update the tests to make sure to handle end-cases in that new corpus.\n",
    "\n",
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
